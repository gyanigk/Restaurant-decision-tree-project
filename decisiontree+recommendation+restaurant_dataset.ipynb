{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "decisiontree+recommendation+restaurant_dataset.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMf2FZpCU3/xAe9B8+xAT1o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/p4rZ/Restaurant-decision-tree-project/blob/main/decisiontree%2Brecommendation%2Brestaurant_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cE5mB1A-10Xs"
      },
      "outputs": [],
      "source": [
        "import pandas as pd, numpy as np\n",
        "import matplotlib.pyplot as plt, seaborn as sns\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "import re\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.svm import SVR\n",
        " \n",
        "# evaluation metrics for classification\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score\n",
        "from sklearn.metrics import recall_score, f1_score, roc_auc_score\n",
        "\n",
        "# evaluation metrics for regression\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#EDA"
      ],
      "metadata": {
        "id": "kxaRxl1yBPUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://storage.googleapis.com/kaggle-data-sets/153420/352891/compressed/zomato.csv.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20220331%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20220331T164154Z&X-Goog-Expires=259199&X-Goog-SignedHeaders=host&X-Goog-Signature=54293a9eea78bd16e53259d485f96c73dcd7e7cd3f65920dca05a1d51a9ca2003726e24e7ada8725f63ba2cfb23641bb5862808af0c1176a773fe3bb80284b18624662caf1225a61725a1eab78448c2c80ddb69832971659aa2a03b39b4e948c50892ecbf172815efb4ceeb9e1d80c5b4cbbf057b6582ee6b2ec0b28ccd6509697e18541e403d619bc6607d10b0ce5497948ea690ce949a91ccc19d9ff59ef6828821b17e2f5b78273e9367490878b4899ae394336adae03ab984bb54c8b224d872581ca54e90407db6480a0eaaffcdcfb76355326fb2dbebc37ef89b4cef4f519392d04de9c60150ad4a266c42e77a709a87a59fcb58bbd221d797741d2ca15\""
      ],
      "metadata": {
        "id": "2Hd8phCOBQyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/zomato.csv.zip"
      ],
      "metadata": {
        "id": "3vtC2OdATxQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zomato = pd.read_csv('/content/zomato.csv')\n",
        "zomato.shape # shape: 51717, 17"
      ],
      "metadata": {
        "id": "2dOViGhoV0nJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rename_cols(data):\n",
        "    data = data.rename(columns = {'approx_cost(for two people)': 'cost_for_two', 'location': 'resto_location'})    \n",
        "    return data"
      ],
      "metadata": {
        "id": "iu-axBeGWF4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zomato = rename_cols(zomato)"
      ],
      "metadata": {
        "id": "QB5b6wI6WRcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(zomato.head(2))"
      ],
      "metadata": {
        "id": "_IPJ4Wl5WWIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the below method will tell us whether there are missing values or not, and if \n",
        "# yes, then which columns have those missing values\n",
        "def missing_values(data):\n",
        "    dat = pd.DataFrame(round(100*data.isnull().sum()/len(data.index), 2)).reset_index().rename(columns = {'index': 'columns', 0: 'missing'})\n",
        "    list(dat[dat.missing>0]['columns'])\n",
        "    if sum(list(dat['missing'])) == 0:\n",
        "        return 'No Missing Value'\n",
        "    else:\n",
        "        return dat[dat.missing>0]"
      ],
      "metadata": {
        "id": "9bZhFAdEWzwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dish_liked has 55% missing values, rate has 15% missing values, phone has 2% \n",
        "# missing values\n",
        "# dish_liked can be used with sentimental analysis, so we cam remove it here\n",
        "# in the regression/classification analysis\n",
        "def treat_missing_values(data, key = ''):\n",
        "    if key == 'cuisines':\n",
        "        data.cuisines = data.cuisines.fillna('')\n",
        "    elif key == 'rate':\n",
        "        data.rate = data.rate.fillna(data.rate.mode()[0])\n",
        "    elif key == 'cost':\n",
        "        data['cost_for_two'] = data['cost_for_two'].fillna('0')\n",
        "    elif key == 'dish':\n",
        "        data['dish_liked'] = data['dish_liked'].fillna('')\n",
        "    elif key == 'resto':\n",
        "        data['resto_location'] = data['resto_location'].fillna(data['resto_location'].mode()[0])\n",
        "        data.rest_type = data.rest_type.fillna(data.rest_type.mode()[0])\n",
        "    else:\n",
        "        pass"
      ],
      "metadata": {
        "id": "eogUPKzUWgT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "treat_missing_values(zomato, key = 'cuisines')\n",
        "treat_missing_values(zomato, key = 'rate')\n",
        "treat_missing_values(zomato, key = 'cost')\n",
        "treat_missing_values(zomato, key = 'dish')\n",
        "treat_missing_values(zomato, key = 'resto')\n",
        "missing_values(zomato)"
      ],
      "metadata": {
        "id": "MxVwqTR5Wo2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sep(x, key = ''):\n",
        "    if key == 'dishes':\n",
        "        if x<6:\n",
        "            return ' less than 5'\n",
        "        elif x>5 and x<11:\n",
        "            return '6-10'\n",
        "        elif x>10:\n",
        "            return 'more than 10'\n",
        "        else: \n",
        "            pass\n",
        "    elif key == 'cuisines':\n",
        "        if x<4:\n",
        "            return 'less than 5'\n",
        "        elif x>3:\n",
        "            return 'more than 5'\n",
        "        else: \n",
        "            pass\n",
        "            \n",
        "# 'location_same' (whether the location of resto and the place where it is listed is same). \n",
        "# We can also # gain two features out of two unnecessary columns: no._of_cuisines, no._of_dishes_liked\n",
        "def feature_extraction(data, key = ''):\n",
        "    if key == 'cuisines':\n",
        "        data['#cuisines'] = data.cuisines.apply(lambda x: sep(len(x.split(' ')), 'cuisines'))\n",
        "    elif key == 'dishes':\n",
        "        data['#dishes_liked'] = data.dish_liked.apply(lambda x: sep(len(x.split(' ')), 'dishes'))\n",
        "    elif key == 'resto':\n",
        "        data['#resto_type'] = data.rest_type.apply(lambda x: len(x.split(',')))\n",
        "    elif key == 'review':\n",
        "        data['#reviews'] = data.reviews_list.apply(lambda x: len(x.split(',')))\n",
        "    elif key == 'same_location':\n",
        "        data['same_location_as_listed?'] = ['Yes' if i==True else 'No' for i in (np.array(data.resto_location)==np.array(data['listed_in(city)']))]\n",
        "    else:\n",
        "        pass"
      ],
      "metadata": {
        "id": "AQ5zVu5JXRKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_extraction(zomato, 'cuisines')\n",
        "feature_extraction(zomato, 'dishes')\n",
        "feature_extraction(zomato, 'resto')\n",
        "feature_extraction(zomato, 'review')\n",
        "feature_extraction(zomato, 'same_location')\n",
        "# zomato[['#cuisines', '#dishes_liked', '#resto_type', '#reviews', 'same_location_as_listed?']]\n",
        "missing_values(zomato)"
      ],
      "metadata": {
        "id": "1tMgEMX0XSZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zomato[['#cuisines', '#dishes_liked', '#resto_type', '#reviews', 'same_location_as_listed?']]"
      ],
      "metadata": {
        "id": "rKzvyqLbYUmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we treat the ratings and convert them into percentages\n",
        "# there seems to have been a shift in the data. \n",
        "# since only 4% of the data is shifted, we can work using the rest of the 96% \n",
        "# and thus we will drop this data which is 4% of the whole data\n",
        "###\n",
        "# round(100*len([i for i in zomato.rest_type if len(i.split(','))==2])/len(zomato), 2)\n",
        "# 15% of the data has rest_type confused. \n",
        "# Below function deals with it by keeping only the first value in a pair\n",
        "def feature_treatment(data, key = ''):\n",
        "    if key == 'drop':\n",
        "        data.drop(labels = list(data.index[data.rate=='NEW']), axis = 0, inplace = True)\n",
        "        data.drop(labels = list(data.index[data.rate=='-']), axis = 0, inplace = True)\n",
        "    elif key == 'rate':\n",
        "        data.rate = data.rate.apply(lambda x: x.replace(' ', ''))\n",
        "        data.rate = data.rate.apply(lambda x: int(20*float(x.split('/')[0])))\n",
        "    elif key == 'cost':\n",
        "        data['cost_for_two'] = data['cost_for_two'].apply(lambda x: int(x.replace(',', '')))\n",
        "    elif key == 'resto': \n",
        "        data.rest_type = data.rest_type.apply(lambda x: x.split(', ')[0])\n",
        "    elif key == 'cuisine':\n",
        "        data.cuisines = data.cuisines.apply(lambda x: x.split(', ')[0])\n",
        "    elif key == 'online_order':\n",
        "        data.online_order = data.online_order.map(dict(Yes=1, No=0))\n",
        "    elif key == 'book_table':\n",
        "        data.book_table = data.book_table.map(dict(Yes=1, No=0))\n",
        "    elif key == 'same_location':\n",
        "        data['same_location_as_listed?'] = data['same_location_as_listed?'].map(dict(Yes=1, No=0))\n",
        "    elif key == 'resto_type':\n",
        "        data['#resto_type'] = data['#resto_type'].apply(lambda x: x-1)\n",
        "    elif key == 'resto_location':\n",
        "        data['resto_location'] = [re.sub(r'^Koramangala..........', 'Koramangala', i) for i in list(data.resto_location)]\n",
        "# https://stackoverflow.com/questions/47517696/replace-strings-in-a-list-using-re-sub\n",
        "    else:\n",
        "        pass"
      ],
      "metadata": {
        "id": "47Ohc7eNXiUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_treatment(zomato, 'drop')\n",
        "feature_treatment(zomato, 'rate')\n",
        "feature_treatment(zomato, 'cost')\n",
        "feature_treatment(zomato, 'resto')\n",
        "feature_treatment(zomato, 'cuisine')\n",
        "feature_treatment(zomato, 'online_order')\n",
        "feature_treatment(zomato, 'book_table')\n",
        "feature_treatment(zomato, 'same_location')\n",
        "feature_treatment(zomato, 'resto_location')\n",
        "feature_treatment(zomato, 'resto_type')\n",
        "# zomato[['cuisines', 'rest_type', 'cost_for_two', 'rate', 'online_order', 'book_table', 'same_location_as_listed?']]\n",
        "# missing_values(zomato)"
      ],
      "metadata": {
        "id": "3rWdpow_X0Xw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zomato[['cuisines', 'rest_type', 'cost_for_two', 'rate', 'online_order', 'book_table', 'same_location_as_listed?']]"
      ],
      "metadata": {
        "id": "SJ-KxVANX7oO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# some features are unneeded, some of which are useful only for sentimental analysis\n",
        "# so we will separate and remove. \n",
        "def delete_unneeded_columns(data):\n",
        "    zomato_2 = data[['dish_liked', 'reviews_list', 'menu_item', 'cuisines']]\n",
        "    data = data.drop(labels = ['url', 'address', 'name', 'phone', 'dish_liked', \n",
        "                               'reviews_list', 'menu_item', 'listed_in(city)'], \n",
        "                                 axis = 1, inplace = True)\n",
        "    return zomato_2"
      ],
      "metadata": {
        "id": "mO7ivWwGYme5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zomato_nlp = delete_unneeded_columns(zomato)"
      ],
      "metadata": {
        "id": "ttmOCQUZYSEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zomato_nlp.head(2)"
      ],
      "metadata": {
        "id": "219FuYp0YvFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# below are the values to be binned in each respective feature. Based on the cumulative\n",
        "# sum from frequency counts.  \n",
        "def binning(data, key = ''):\n",
        "    list_restTypes_exc = list(pd.DataFrame(100*zomato['rest_type'].value_counts()/len(zomato)).cumsum(axis = 0)[pd.DataFrame(100*zomato['rest_type'].value_counts()/len(zomato)).cumsum(axis = 0)['rest_type']>95].index)\n",
        "    list_locs_exc = list(pd.DataFrame(100*zomato['resto_location'].value_counts()/len(zomato)).cumsum(axis = 0)[pd.DataFrame(100*zomato['resto_location'].value_counts()/len(zomato)).cumsum(axis = 0)['resto_location']>80].index)\n",
        "    list_cuisines_exc = list(pd.DataFrame(100*zomato['cuisines'].value_counts()/len(zomato)).cumsum(axis = 0)[pd.DataFrame(100*zomato['cuisines'].value_counts()/len(zomato)).cumsum(axis = 0)['cuisines']>85].index)\n",
        "    def resto(x):\n",
        "        if x in list_restTypes_exc:\n",
        "            return zomato.rest_type.mode()[0]\n",
        "        else:\n",
        "            return x\n",
        "    def location(x):\n",
        "        if x in list_locs_exc:\n",
        "            return zomato.resto_location.mode()[0]\n",
        "        else:\n",
        "            return x\n",
        "    def cuisines(x):\n",
        "        if x in list_cuisines_exc:\n",
        "            return zomato.cuisines.mode()[0]\n",
        "        else:\n",
        "            return x\n",
        "    if key == 'resto':\n",
        "        data.rest_type = data.rest_type.apply(resto)\n",
        "    elif key == 'location':\n",
        "        data.resto_location = data.resto_location.apply(location)\n",
        "    elif key == 'cuisine':\n",
        "        data['cuisines'] = data.cuisines.apply(cuisines)\n",
        "    else:\n",
        "        pass"
      ],
      "metadata": {
        "id": "PZV4dO0rZ3tD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we are left with only 80% of our original data now\n",
        "binning(zomato, 'resto')\n",
        "binning(zomato, 'location')\n",
        "binning(zomato, 'cuisine')"
      ],
      "metadata": {
        "id": "268pN4lAZ8Jh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def outlier_handling(data, key = ''):\n",
        "    if key == 'cost':        \n",
        "        l = np.percentile(data.cost_for_two, 25)-(1.5*(np.percentile(data.cost_for_two, 75)-np.percentile(data.cost_for_two, 25)))\n",
        "        u = np.percentile(data.cost_for_two, 75)+(1.5*(np.percentile(data.cost_for_two, 75)-np.percentile(data.cost_for_two, 25)))    \n",
        "        data.cost_for_two = [data.cost_for_two.median() if ((i<l) or (i>u)) else i for i in data.cost_for_two]\n",
        "    elif key == 'reviews':\n",
        "        # 17 is the median value. lower limit is -63 and upper limit is 124\n",
        "        data['#reviews'] = np.where(data['#reviews'] > 124, 17, data['#reviews'])\n",
        "    elif key == 'votes':\n",
        "        # 73 is the median value. lower limit is -361 and upper limit is 658\n",
        "        data['votes'] = np.where(data['votes'] > 658, 73, data['votes'])"
      ],
      "metadata": {
        "id": "h5Hp8Ytna3p3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outlier_handling(zomato, 'cost')\n",
        "outlier_handling(zomato, 'reviews')\n",
        "outlier_handling(zomato, 'votes')"
      ],
      "metadata": {
        "id": "xO_vVRioZ_8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# should I use get_dummies or OHE? https://bit.ly/3Kaa1VO\n",
        "# can't proceed with encoding without imbalance data handling\n",
        "def encoding(data):\n",
        "    data_cat = data.select_dtypes(include = 'object')\n",
        "    li = data.select_dtypes(include = 'object').columns\n",
        "    enc = OneHotEncoder(sparse = False)\n",
        "    data_cat = pd.DataFrame(enc.fit_transform(data_cat))\n",
        "    data_cat.columns = enc.get_feature_names_out(li)\n",
        "    data_int = data.select_dtypes(include = ['int64', 'float64'])\n",
        "    data_int.reset_index(drop=True, inplace=True)\n",
        "    data_cat.reset_index(drop=True, inplace=True)\n",
        "# https://stackoverflow.com/questions/40339886/pandas-concat-generates-nan-values\n",
        "    data = pd.concat([data_int, data_cat], axis = 1)\n",
        "# https://stackoverflow.com/questions/54570947/feature-names-from-onehotencoder\n",
        "    return data"
      ],
      "metadata": {
        "id": "XZEp4YQTbL18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zomato = encoding(zomato)"
      ],
      "metadata": {
        "id": "RqxOOwdubU14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zomato.head(10)"
      ],
      "metadata": {
        "id": "F7nqhm7bbcOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zomato.columns"
      ],
      "metadata": {
        "id": "3tl10hzveB9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zomato.shape"
      ],
      "metadata": {
        "id": "c4aY8CWpbvfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def splitting(data):\n",
        "    y = data['rate']\n",
        "    X = data[list(set(zomato.columns) - set(['rate']))]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
        "    return X_train, X_test, y_train, y_test\n"
      ],
      "metadata": {
        "id": "QqewxPJebxJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xtrain, xtest, ytrain, ytest = splitting(zomato)\n",
        "missing_values(xtrain)\n",
        "missing_values(xtest)"
      ],
      "metadata": {
        "id": "rI8QChtZcNc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_Cp8PyVJceBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FEATURE SCALING NEEDS TO BE DONE: for rate, votes, cost_for_two\n",
        "def feature_scaling(train, test):\n",
        "# cannot do scaling of test data without fitting on train data first so input\n",
        "# here will be both X_train and X_test\n",
        "# cannot do feature scaling before splitting the data into train and test\n",
        "# we will use MinMaxScaling for rate, votes, and cost_for_two. Normal distribution\n",
        "# is only for 'rate' column, not for the rest. If it is not a normal distribution\n",
        "# then we should go for minmaxscaling. \n",
        "# https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/\n",
        "    minmax = MinMaxScaler()\n",
        "    train[['votes', 'cost_for_two', '#reviews']] = minmax.fit_transform(train[['votes', 'cost_for_two', '#reviews']])\n",
        "    test[['votes', 'cost_for_two', '#reviews']] = minmax.transform(test[['votes', 'cost_for_two', '#reviews']])\n",
        "    return train, test"
      ],
      "metadata": {
        "id": "BxgWxxQHcT7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaled_train_X, scaled_test_X = feature_scaling(xtrain, xtest)\n",
        "missing_values(scaled_train_X)\n",
        "missing_values(scaled_test_X)"
      ],
      "metadata": {
        "id": "-TGL9JxBdOmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaled_train_X.head(10)"
      ],
      "metadata": {
        "id": "wRq18mg9dVYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaled_test_X.head(10)"
      ],
      "metadata": {
        "id": "QchzdWqte6gS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DIMENSIONALITY REDUCTION? PCA or RFE? RFE\n",
        "#def reduction(data):\n",
        "#    rfe = RFE(data)\n",
        "#    pass"
      ],
      "metadata": {
        "id": "dswWKiIidwfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reduction(zomato)"
      ],
      "metadata": {
        "id": "rLND3Z3ZdxI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install graphviz"
      ],
      "metadata": {
        "id": "LJ4NbxfTg1Zr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydotplus"
      ],
      "metadata": {
        "id": "phj8ES6tg4DP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import tree\n",
        "import graphviz\n",
        "import pydotplus\n",
        "from IPython.display import Image"
      ],
      "metadata": {
        "id": "curTIqpMhMmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# without feature selection, we tried fitting the models on our model and eval\n",
        "# linreg gave 45% MSE, Random Forest 8%, Decision Tree 12%. So we will use both\n",
        "# random forest regressor and decision trees\n",
        "def regression_modelling(X_train, X_test, y_train, key = ''):\n",
        "    if key == 'decisiontrees':\n",
        "        model = tree.DecisionTreeRegressor(max_depth=4)\n",
        "        model.fit(X_train, y_train)\n",
        "        pred = model.predict(X_test)\n",
        "\n",
        "        dot = tree.export_graphviz(model, out_file = None,\n",
        "                          feature_names = X_train.columns,filled=True)\n",
        "\n",
        "        graph = pydotplus.graph_from_dot_data(dot)\n",
        "        Image(graph.create_png())\n",
        "        return pred\n",
        "    else:\n",
        "        pass"
      ],
      "metadata": {
        "id": "uKFavHlffS2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dtr_pred = list(regression_modelling(scaled_train_X, scaled_test_X, ytrain, 'decisiontrees'))"
      ],
      "metadata": {
        "id": "3MZ0QlYafNPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tree.DecisionTreeRegressor(max_depth=2,criterion=\"gini\")\n",
        "model.fit(scaled_train_X, ytrain)\n",
        "pred = model.predict(scaled_test_X)\n",
        "\n",
        "dot = tree.export_graphviz(model, out_file = None,feature_names = scaled_train_X.columns,filled=True)\n",
        "\n",
        "graph = pydotplus.graph_from_dot_data(dot)\n",
        "Image(graph.create_png())"
      ],
      "metadata": {
        "id": "VLGTmpNYhYUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tree.DecisionTreeRegressor(max_depth=3)\n",
        "model.fit(scaled_train_X, ytrain)\n",
        "pred = model.predict(scaled_test_X)\n",
        "\n",
        "dot = tree.export_graphviz(model, out_file = None,feature_names = scaled_train_X.columns,filled=True)\n",
        "\n",
        "graph = pydotplus.graph_from_dot_data(dot)\n",
        "Image(graph.create_png())"
      ],
      "metadata": {
        "id": "zKxPdBkOgZAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tree.DecisionTreeRegressor(max_depth=4)\n",
        "model.fit(scaled_train_X, ytrain)\n",
        "pred = model.predict(scaled_test_X)\n",
        "\n",
        "dot = tree.export_graphviz(model, out_file = None,feature_names = scaled_train_X.columns,filled=True)\n",
        "\n",
        "graph = pydotplus.graph_from_dot_data(dot)\n",
        "Image(graph.create_png())"
      ],
      "metadata": {
        "id": "R_Sbel09h3Zf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tree.DecisionTreeRegressor(max_depth=5)\n",
        "model.fit(scaled_train_X, ytrain)\n",
        "pred = model.predict(scaled_test_X)\n",
        "\n",
        "dot = tree.export_graphviz(model, out_file = None,feature_names = scaled_train_X.columns,filled=True)\n",
        "\n",
        "graph = pydotplus.graph_from_dot_data(dot)\n",
        "Image(graph.create_png())"
      ],
      "metadata": {
        "id": "pauudfVSh7Nn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"max_depth     accuracy\")\n",
        "for x in range(1,21):\n",
        "    clf = tree.DecisionTreeClassifier(max_depth = x,criterion = \"gini\")\n",
        "    clf = clf.fit(scaled_train_X, ytrain)\n",
        "    y_pred = clf.predict(scaled_test_X)\n",
        "    cm = confusion_matrix(ytest,y_pred)\n",
        "    accuracy = accuracy_score(ytest,y_pred)\n",
        "    print(x,\"          \",accuracy)"
      ],
      "metadata": {
        "id": "3oVFM5OyjnXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"max_depth     accuracy\")\n",
        "for x in range(1,21):\n",
        "    clf = tree.DecisionTreeClassifier(max_depth = x,criterion = \"entropy\")\n",
        "    clf = clf.fit(scaled_train_X, ytrain)\n",
        "    y_pred = clf.predict(scaled_test_X)\n",
        "    cm = confusion_matrix(ytest,y_pred)\n",
        "    accuracy = accuracy_score(ytest,y_pred)\n",
        "    print(x,\"          \",accuracy)\n",
        "\n",
        "\n",
        "print(\"confusion matrix\")\n",
        "print(cm)"
      ],
      "metadata": {
        "id": "G1cPcoXkj3yd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(ytest,y_pred)\n",
        "accuracy_gini = accuracy_score(ytest,y_pred)\n",
        "print(\"confusion matrix\")\n",
        "print(cm)\n",
        "print(\"Accuracy = \",accuracy_gini)"
      ],
      "metadata": {
        "id": "Gv3BAr7ekgnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (10,10), dpi=1000)\n",
        "tree.plot_tree(clf,filled=True,fontsize=3,max_depth=3);\n",
        "fig.savefig('maxdepth3.png')"
      ],
      "metadata": {
        "id": "nv8WUJ5fmvtF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# correlation analysis\n",
        "# rating depends on number of dishes liked, and votes. STILL NEED: will make \n",
        "# two scenarios: one where threshold is 0.58 so I remove the dishes_liked variable\n",
        "# and one where I don't. Evidently after some preprocessing, the corr results aren't\n",
        "# the same anymore so no need to do correlation analysis\n",
        "\n",
        "# IMBALANCE DATA ANALYSIS DONE using simple value_counts divided by 100:\n",
        "# 'rest_type', 'listed_in(type)', 'cuisines' have imbalance. 'location' and \n",
        "# 'listed_in(city)' though does not have a huge imbalance, there are still many \n",
        "# values that have really less frequency, so for that, to reduce dimensionality \n",
        "# here, we can club those rarer values into a single 'unknown' category'. \n",
        "# 'listed_in(city)' has four values with 'Koramangala' in them. Will group them so the\n",
        "# data will become imbalanced. Will have to see about it. STILL NEED: treat the imbalance\n",
        "# we will do two things here: we will do the modelling with and without imbalance both \n",
        "# and compare the performances\n",
        "\n",
        "# NORMALITY ANALYSIS DONE USING BOXPLOT: votes, cost_for_two, number_of_cuisines,\n",
        "# number_of_dishes_liked are not normal.\n",
        "\n"
      ],
      "metadata": {
        "id": "Do7QNlLYYzLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf = tree.DecisionTreeClassifier(max_depth = 22,criterion = \"entropy\")\n",
        "clf = clf.fit(scaled_train_X, ytrain)\n",
        "y_pred = clf.predict(scaled_test_X)"
      ],
      "metadata": {
        "id": "9srJvYu5oe2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "51HTLRlPpOgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluation(pred, test, key = ''):\n",
        "    if key == 'accuracy':\n",
        "        acc = accuracy_score(ytest,y_pred)\n",
        "        print(\"Accuracy \\n\",acc,\"\\n\") \n",
        "    elif key == 'confusionMatrix':\n",
        "        cm = confusion_matrix(ytest,y_pred)\n",
        "        # plot_confusion_matrix(clf, ytest,y_pred)\n",
        "        # plt.show()\n",
        "        disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=clf.classes_)\n",
        "        disp.plot()\n",
        "        plt.show()\n",
        "        # print(\"Confusion Matrix\\n\",cm,\"\\n\") \n",
        "    elif key == 'precision':\n",
        "        p1 = precision_score(ytest, y_pred, average='macro')\n",
        "        p2 = precision_score(ytest, y_pred, average='micro')\n",
        "        p3 = precision_score(ytest, y_pred, average='weighted')\n",
        "        # p4 = precision_score(ytest, y_pred, average=None)\n",
        "        print(\"Precision \\n MACRO :\",p1,\"\\n MICRO :\",p2,\"\\n WEIGHTED :\",p3,\"\\n\") \n",
        "    elif key == 'recall':\n",
        "        r1 = recall_score(ytest, y_pred, average='macro')\n",
        "        r2 = recall_score(ytest, y_pred, average='micro')\n",
        "        r3 = recall_score(ytest, y_pred, average='weighted')\n",
        "        # r4 = recall_score(ytest, y_pred, average=None)\n",
        "        print(\"Recall \\n MACRO :\",r1,\"\\n MICRO :\",r2,\"\\n WEIGHTED :\",r3,\"\\n\") \n",
        "    elif key == 'f1':\n",
        "        f1 = f1_score(ytest, y_pred, average='macro')\n",
        "        f2 = f1_score(ytest, y_pred, average='micro')\n",
        "        f3 = f1_score(ytest, y_pred, average='weighted')\n",
        "        # f4 = f1_score(ytest, y_pred, average=None)\n",
        "        print(\"F1 score \\n MACRO :\",f1,\"\\n MICRO :\",f2,\"\\n WEIGHTED :\",f3,\"\\n\")  \n",
        "    if key == 'mse':\n",
        "        err = mean_squared_error(pred, test)\n",
        "        print(\"MSE \\n\",round(err, 2),\"\\n\")     \n",
        "    else: pass"
      ],
      "metadata": {
        "id": "c43JGtGzoVjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation(dtr_pred, list(ytest), 'accuracy')\n",
        "evaluation(dtr_pred, list(ytest), 'confusionMatrix')\n",
        "evaluation(dtr_pred, list(ytest), 'precision')\n",
        "evaluation(dtr_pred, list(ytest), 'recall')\n",
        "evaluation(dtr_pred, list(ytest), 'f1')\n",
        "evaluation(dtr_pred, list(ytest), 'mse')"
      ],
      "metadata": {
        "id": "sh3ArioloYIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.close(all)\n",
        "# fig.close()"
      ],
      "metadata": {
        "id": "cpEUzK1OyDud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cm = confusion_matrix(ytest,y_pred)\n",
        "# plot_confusion_matrix(clf, ytest,y_pred)\n",
        "# plt.show()\n",
        "fig = plt.figure()\n",
        "plt.figure().clear()\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=clf.classes_,)\n",
        "disp.plot(ax=fig.add_axes([-2, -2, 2, 2]))\n",
        "fig.savefig('final_cm_1.png', dpi=100,pad_inches = 1,orientation ='landscape')\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "qDGZGlyJtu2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "EzkjnAX8128o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}